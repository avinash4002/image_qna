
# Image Question Answering with BLIP

This project demonstrates how to use the BLIP (Bootstrapped Language-Image Pretraining) model for image question answering. The model is deployed on Hugging Face Spaces using Gradio.

## Project Overview

This repository contains code to deploy a BLIP-based image question answering model. The model is capable of answering questions about images. The project uses Gradio for the interactive web interface and Hugging Face Spaces for hosting the application.

## How to Use

1. **Clone the Repository**

   ```bash
   git clone https://github.com/avinash4002/image_qna.git
   cd image_qna
Install Dependencies

Make sure you have Python 3.7 or higher installed. Install the required packages using requirements.txt:

'''bash
pip install -r requirements.txt

Run the Application Locally

To run the Gradio application locally, use the following command:

'''bash
python app.py
This will start a local server, and you can interact with the model through your browser.

Deployed Model on Hugging Face Spaces
The model has been deployed and is accessible on Hugging Face Spaces. You can interact with the deployed application via the following link:
https://huggingface.co/spaces/786avinash/que_ans


Project Structure
app.py: The main script that initializes the Gradio interface and loads the model.
requirements.txt: Lists all the dependencies required to run the project.
README.md: This file, providing an overview and instructions for the project.
Contributing
Feel free to fork the repository and make improvements. If you have suggestions or find issues, please open an issue or submit a pull request.

License
This project is licensed under the MIT License - see the LICENSE file for details.

Acknowledgments
Hugging Face for the Spaces platform and the BLIP model.
Gradio for providing an easy-to-use interface for machine learning models.



